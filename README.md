#  **PySpark SQL Unit Test Framework**

**Automated SQL Unit Testing with CSV Inputs, Multi-Case Support, Schema Inference & PySpark Execution**

![alt text](images/Unit_testing_for_sql_flow.png)
![alt text](images/workflow.png)

---

# **Overview**

This framework provides **fully automated unit testing for SQL logic executed on PySpark**.
It allows teams to validate SQL transformations using **lightweight CSV input files**, powerful **schema inference**, and **auto-generated pytest tests** with bundled multi-scenario support.

The goal:

> **Zero manual efforts. You write SQL â†’ The framework generates tests + wrappers automatically.**

The system supports:

* SQL files located **anywhere** in the repository
* CSV inputs (default + multiple case files)
* **Schema inference & normalization** (ensuring SQL never fails due to CSV typing issues)
* Fully-qualified table names (`db.schema.table`)
* Auto-registration of SQL tables as views
* Bundled scenario tests
* Optional expected outputs (golden datasets)
* GitLab CI pipeline support

---

# **High-Level Architecture**

```
SQL Files â†’ Generator â†’ Wrappers â†’ Tests â†’ Pytest + Spark â†’ Compare Results â†’ Reports/CI
```

---

# ğŸ— **Folder Structure**

```
your_repo/
â”‚
â”œâ”€â”€ setup_framework.py                â† One-time generator script
â”‚
â”œâ”€â”€ sql/                              â† SQL source files (optional default root)
â”‚     â”œâ”€â”€ calc_revenue.sql
â”‚     â”œâ”€â”€ finance/
â”‚     â”‚      â””â”€â”€ rollup.sql
â”‚     â””â”€â”€ hr/
â”‚            â””â”€â”€ employee_pay.sql
â”‚
â”œâ”€â”€ generator/
â”‚     â””â”€â”€ generate_framework.py       â† Auto-created generator
â”‚
â”œâ”€â”€ utils/                            â† Utility modules (assertions, loaders, parsers)
â”‚     â”œâ”€â”€ __init__.py
â”‚     â”œâ”€â”€ sql_loader.py
â”‚     â”œâ”€â”€ sql_table_parser.py
â”‚     â”œâ”€â”€ data_loader.py
â”‚     â”œâ”€â”€ csv_schema_resolver.py
â”‚     â””â”€â”€ assertions.py               â† Numeric tolerant comparator (int/long/float/double)
â”‚
â”œâ”€â”€ wrappers/                         â† Auto-generated wrappers (DO NOT EDIT MANUALLY)
â”‚     â”œâ”€â”€ src_finance_calc_revenue.py
â”‚     â”œâ”€â”€ finance_rollup.py
â”‚     â””â”€â”€ hr_employee_pay.py
â”‚
â”œâ”€â”€ tests/                            â† Auto-generated test files + your Python tests
â”‚     â”œâ”€â”€ test_src_finance_calc_revenue.py
â”‚     â”œâ”€â”€ test_finance_rollup.py
â”‚     â”œâ”€â”€ test_hr_employee_pay.py
â”‚     â”œâ”€â”€ test_logic1.py              â† Your own Python tests
â”‚     â””â”€â”€ test_logic2.py
â”‚
â”œâ”€â”€ test_data/                        â† Input data + expected outputs for each SQL file
â”‚     â”œâ”€â”€ src/finance/calc_revenue/
â”‚     â”‚       â”œâ”€â”€ employees.csv                 â† input table
â”‚     â”‚       â”œâ”€â”€ departments.csv               â† input table
â”‚     â”‚       â”œâ”€â”€ employees_case1.csv           â† case-specific input
â”‚     â”‚       â”œâ”€â”€ expected_default.csv          â† expected output for default
â”‚     â”‚       â”œâ”€â”€ expected_case1.csv            â† expected output for case1
â”‚     â”‚       â””â”€â”€ expected_case2.csv            â† etc.
â”‚     â”‚
â”‚     â”œâ”€â”€ finance/rollup/
â”‚     â”‚       â”œâ”€â”€ orders.csv
â”‚     â”‚       â”œâ”€â”€ expected_default.csv
â”‚     â”‚       â””â”€â”€ expected_case1.csv
â”‚     â”‚
â”‚     â””â”€â”€ hr/employee_pay/
â”‚             â”œâ”€â”€ salary.csv
â”‚             â”œâ”€â”€ bonus.csv
â”‚             â””â”€â”€ expected_default.csv
â”‚
â”œâ”€â”€ conftest.py                        â† Shared Spark session for all tests
â”‚
â”œâ”€â”€ htmlcov/                           â† Coverage HTML output (created after pytest)
â”‚
â””â”€â”€ coverage.xml                       â† XML coverage output for CI/CD

```

---

#  **Key Features**

## 1ï¸âƒ£ **SQL Auto-Discovery**

SQL files can be anywhere in the repo.

You can run the generator in two modes:

### Scan only specific folder

```
python generator/generate_framework.py --sql-root sql
```

### Scan entire repository

```
python generator/generate_framework.py --scan-all
```

The framework extracts:

* Table names
* Fully-qualified names
* Module key
* Folder structure for tests and inputs

---

## 2ï¸âƒ£ **Auto-Generated Python Wrappers**

Each SQL file becomes a Python function:

```
def run_filter_employees(spark):
    sql = load_sql("src/etl/filter_employees.sql")
    return spark.sql(sql)
```

Stored under:

```
wrappers/<module_key>.py
```

---

## 3ï¸âƒ£ **CSV Input-Driven Testing (Primary Input Format)**

### Default CSV per table:

```
employees.csv
departments.csv
```

### Multi-case scenario CSVs:

```
employees_case1.csv
employees_case2.csv
departments_case1.csv
```

### Missing case fallback:

Case missing? â†’ automatically uses default CSV.

The framework NEVER skips a case due to missing case files.

---

## 4ï¸âƒ£ **Schema Inference + Normalization**

CSV files are freeform (string-based).
Framework automatically:

* Infers column types
* Fixes inconsistent types
* Detects booleans, numbers, timestamps
* Adds missing columns
* Drops extra columns
* Ensures join keys match across tables
* Guarantees SQL can run without schema errors

This allows easy editing of CSVs without worrying about Spark typing rules.

---

## 5ï¸âƒ£ **View Registration (Base + Fully-Qualified)**

If SQL contains:

```
SELECT * FROM db1.hr.employees
```

The framework registers both:

```
employees
`db1.hr.employees`
```

This ensures SQL runs unchanged.

---

## 6ï¸âƒ£ **Generated Tests**

Two types:

### **A. Per-Table Tests**

Each table + each input case:

```
test_filter_employees_employees_default
test_filter_employees_employees_case1
test_filter_employees_departments_case1
```

Each test:

* Loads CSV input
* Normalizes schema
* Registers table view
* Runs SQL wrapper
* Asserts basic correctness

### **B. Bundled Scenario Tests**

Case ID = union of all table-specific cases.

Example:

* employees has case1
* departments has only default

Then:

```
bundle_case1 = employees_case1 + departments.csv
```

Test runs SQL with combined mapping.

---

## 7ï¸âƒ£ **Optional Expected Output (Golden Dataset)**

Expected output files live under:

```
expected/<module_key>/
```

Example:

```
expected/src/etl/filter_employees/output.csv
expected/src/etl/filter_employees/output_case1.csv
```

If expected CSV exists:

* Load expected
* Normalize schema
* Sort both DFs
* Compare equality

If expected missing:

* Test still runs
* Output validation is skipped
* Warn developer:

```
WARNING: No expected output defined. Create expected/<module_key>/output.csv
```

---

## 8ï¸âƒ£ **Global SparkSession (conftest.py)**

`conftest.py` creates:

* One Spark session for all tests (fast!)
* Preloads default CSV inputs for all SQL files
* Registers views **before** tests run

This accelerates testing, especially in CI pipelines.

---

## 9ï¸âƒ£ **GitLab CI Integration**

`.gitlab-ci.yml` supports:

* Installing dependencies
* Running generator
* Running pytest
* Publishing test artifacts

---

# ğŸš€ **Using the Framework**

---

## **1. Install dependencies**

```
pip install -r requirements.txt
```

---

## **2. Place SQL files anywhere**

Example:

```
src/etl/hr/filter_employees.sql
dags/sql/join_salary.sql
```

---

## **3. Run Generator**

### To scan a specific folder:

```
python generator/generate_framework.py --sql-root src/etl/hr
```

### To scan entire repository:

```
python generator/generate_framework.py --scan-all
```

Generator will produce:

* wrappers/
* tests/
* test_data/
* expected/
* test_map.yaml

---

## **4. Provide CSV Inputs**

Under:

```
test_data/<module_key>/
```

Add:

```
employees.csv
employees_case1.csv
departments.csv
```

---

## **5. Add Expected Output (Optional)**

```
expected/<module_key>/output.csv
```

---

## **6. Run Tests**

```
pytest -q
```

You will see:

* Per-table tests
* Bundled tests
* Expected comparison tests

---

# ğŸ§  **How Schema Inference Works**

The framework samples CSV rows to guess column types:

| Value Example   | Inferred Type |
| --------------- | ------------- |
| `123`           | INT           |
| `12.5`          | DOUBLE        |
| `true`, `false` | BOOLEAN       |
| `2024-01-01`    | TIMESTAMP     |
| anything else   | STRING        |

Then it casts columns to consistent Spark types before running SQL.

---

# ğŸ” **Example Scenario**

SQL file:

```
SELECT e.emp_id, d.dept_name
FROM employees e
JOIN departments d ON e.dept_id = d.dept_id
```

CSV input:

```
test_data/src/etl/filter_employees/
    employees.csv
    employees_case1.csv
    departments.csv
```

Generated tests:

```
test_filter_employees_per_table
test_filter_employees_bundle_case1
```

---

# ğŸ§© **Extensibility**

You can:

* Switch CSV to Parquet
* Add custom type inference
* Add expected output mapping from metadata CSV
* Extend SQL parsing
* Integrate with dbt or Airflow

---

# ğŸ“š **Troubleshooting**

| Issue                            | Cause                | Solution                             |
| -------------------------------- | -------------------- | ------------------------------------ |
| SQL fails with â€œtable not foundâ€ | Missing CSV input    | Add `<table>.csv`                    |
| Schema mismatch                  | CSV typing issue     | Schema inference handles; add header |
| No expected validation           | Missing expected CSV | Add under expected/<module_key>/     |
| No SQL discovered                | Wrong root folder    | Use `--scan-all`                     |

---

# ğŸ **Conclusion**

This framework gives you **powerful, automated, data-driven SQL testing** without writing manual test code.
Simply:

* Write SQL
* Prepare CSV inputs
* Run tests

Everything else is auto-generated and executed under PySpark.

---

